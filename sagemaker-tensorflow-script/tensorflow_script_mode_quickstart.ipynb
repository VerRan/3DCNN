{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow Scripts in SageMaker - Quickstart\n",
    "\n",
    "Starting with TensorFlow version 1.11, you can use SageMaker's TensorFlow containers to train TensorFlow scripts the same way you would train outside SageMaker. This feature is named **Script Mode**. \n",
    "\n",
    "This example uses \n",
    "[Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow). \n",
    "You can use the same technique for other scripts or repositories, including \n",
    "[TensorFlow Model Zoo](https://github.com/tensorflow/models) and \n",
    "[TensorFlow benchmark scripts](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "For training data, we use plain text versions of Sherlock Holmes stories.\n",
    "Let's create a folder named **sherlock** to store our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), 'sherlock')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the dataset to this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-16 08:04:36--  https://sherlock-holm.es/stories/plain-text/cnus.txt\n",
      "Resolving sherlock-holm.es (sherlock-holm.es)... 116.202.106.56, 2a01:4f8:c010:1294::2\n",
      "Connecting to sherlock-holm.es (sherlock-holm.es)|116.202.106.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3382026 (3.2M) [text/plain]\n",
      "Saving to: ‘sherlock/input.txt’\n",
      "\n",
      "sherlock/input.txt  100%[===================>]   3.22M  2.67MB/s    in 1.2s    \n",
      "\n",
      "2020-12-16 08:04:38 (2.67 MB/s) - ‘sherlock/input.txt’ saved [3382026/3382026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://sherlock-holm.es/stories/plain-text/cnus.txt --force-directories --output-document=sherlock/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training script\n",
    "\n",
    "For training scripts, let's use Git integration for SageMaker Python SDK here. That is, you can specify a training script that is stored in a GitHub, CodeCommit or other Git repository as the entry point for the estimator, so that you don't have to download the scripts locally. If you do so, source directory and dependencies should be in the same repo if they are needed.\n",
    "\n",
    "To use Git integration, pass a dict `git_config` as a parameter when you create the `TensorFlow` Estimator object. In the `git_config` parameter, you specify the fields `repo`, `branch` and `commit` to locate the specific repo you want to use. If authentication is required to access the repo, you can specify fields `2FA_enabled`, `username`, `password` and token accordingly.\n",
    "\n",
    "The scripts we want to use for this example is stored in GitHub repo \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts](https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts), \n",
    "under the branch `training-scripts`. It is a public repo so we don't need authentication to access it. Let's specify the `git_config` argument here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/awslabs/amazon-sagemaker-examples.git', 'branch': 'training-scripts'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not specify `commit` in `git_config` here, so the latest commit of the specified repo and branch will be used by default. \n",
    "\n",
    "The scripts we will use are under the `char-rnn-tensorflow` directory in the repo. The directory also includes a [README.md](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/README.md#basic-usage) with an overview of the project, requirements, and basic usage:\n",
    "\n",
    "> #### **Basic Usage**\n",
    "> _To train with default parameters on the tinyshakespeare corpus, run **python train.py**. \n",
    "To access all the parameters use **python train.py --help.**_\n",
    "\n",
    "[train.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/char-rnn-tensorflow/train.py#L11) uses the [argparse](https://docs.python.org/3/library/argparse.html) library and requires the following arguments:\n",
    "\n",
    "```python\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare', help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='save', help='directory to store checkpointed models')\n",
    "...\n",
    "args = parser.parse_args()\n",
    "\n",
    "```\n",
    "When SageMaker training finishes, it deletes all data generated inside the container with exception of the directories `_/opt/ml/model_` and `_/opt/ml/output_`. To ensure that model data is not lost during training, training scripts are invoked in SageMaker with an additional argument `--model_dir`. The training script should save the model data that results from the training job to this directory..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script executes in the container as shown bellow:\n",
    "\n",
    "```bash\n",
    "python train.py --num-epochs 1 --data_dir /opt/ml/input/data/training --model_dir /opt/ml/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally using SageMaker Python SDK TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the SageMaker Python SDK [`TensorFlow`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst#training-with-tensorflow) estimator to easily train locally and in SageMaker. \n",
    "\n",
    "Let's start by setting the training script arguments `--num_epochs` and `--data_dir` as hyperparameters. Remember that we don't need to provide `--model_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'num_epochs': 1, 'data_dir': '/opt/ml/input/data/training'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments. Just change your estimator's train_instance_type to local or local_gpu. For more information, see: https://github.com/aws/sagemaker-python-sdk#local-mode.\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU). Running following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you.\n",
    "\n",
    "Note, you can only run a single local notebook at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./setup.sh: line 3: sudo: command not found\n",
      "The user does not have root access. Everything required to run the notebook is already installed and setup. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train locally, you set `train_instance_type` to [local](https://github.com/aws/sagemaker-python-sdk#local-mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type='local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `TensorFlow` Estimator, passing the `git_config` argument and the flag `script_mode=True`. Note that we are using Git integration here, so `source_dir` should be a relative path inside the Git repo; otherwise it should be a relative or absolute local path. the `Tensorflow` Estimator is created as following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='char-rnn-tensorflow',\n",
    "                       git_config=git_config,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a training job, we call `estimator.fit(inputs)`, where inputs is a dictionary where the keys, named **channels**, \n",
    "have values pointing to the data location. `estimator.fit(inputs)` downloads the TensorFlow container with TensorFlow Python 3, CPU version, locally and simulates a SageMaker training job. \n",
    "When training starts, the TensorFlow container executes **train.py**, passing `hyperparameters` and `model_dir` as script arguments, executing the example as follows:\n",
    "```bash\n",
    "python -m train --num-epochs 1 --data_dir /opt/ml/input/data/training --model_dir /opt/ml/model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'docker-compose' is not installed. Local Mode features will not work without docker-compose. For more information on how to install 'docker-compose', please, see https://docs.docker.com/compose/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-274879a8d731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'file://{data_dir}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config, run_tensorboard_locally)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit_super\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_tensorboard_locally\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     def process(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mResourceConfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InstanceCount\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mAlgorithmSpecification\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TrainingImage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalTrainingJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instance_type, instance_count, image, sagemaker_session)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfind_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker-compose\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise ImportError(\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;34m\"'docker-compose' is not installed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;34m\"Local Mode features will not work without docker-compose. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;34m\"For more information on how to install 'docker-compose', please, see \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: 'docker-compose' is not installed. Local Mode features will not work without docker-compose. For more information on how to install 'docker-compose', please, see https://docs.docker.com/compose/install/"
     ]
    }
   ],
   "source": [
    "inputs = {'training': f'file://{data_dir}'}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the values of `--data_dir` and `--model_dir` with more details:\n",
    "\n",
    "- **/opt/ml/input/data/training** is the directory inside the container where the training data is downloaded. The data is downloaded to this folder because `training` is the channel name defined in ```estimator.fit({'training': inputs})```. See [training data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-trainingdata) for more information. \n",
    "\n",
    "- **/opt/ml/model** use this directory to save models, checkpoints, or any other data. Any data saved in this folder is saved in the S3 bucket defined for training. See [model data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-envvariables) for more information.\n",
    "\n",
    "### Reading additional information from the container\n",
    "\n",
    "Often, a user script needs additional information from the container that is not available in ```hyperparameters```.\n",
    "SageMaker containers write this information as **environment variables** that are available inside the script.\n",
    "\n",
    "For example, the example above can read information about the `training` channel provided in the training job request by adding the environment variable `SM_CHANNEL_TRAINING` as the default value for the `--data_dir` argument:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # reads input channels training and testing from the environment variables\n",
    "  parser.add_argument('--data_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "```\n",
    "\n",
    "Script mode displays the list of available environment variables in the training logs. You can find the [entire list here](https://github.com/aws/sagemaker-containers/blob/master/README.rst#list-of-provided-environment-variables-by-sagemaker-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you test the training job locally, upload the dataset to an S3 bucket so SageMaker can access the data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "inputs = sagemaker.Session().upload_data(path='sherlock', key_prefix='datasets/sherlock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned variable inputs above is a string with a S3 location which SageMaker Tranining has permissions\n",
    "to read data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-517141035927/datasets/sherlock'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train in SageMaker:\n",
    "- change the estimator argument `train_instance_type` to any SageMaker ml instance available for training.\n",
    "- set the `training` channel to a S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-16 08:17:26 Starting - Starting the training job...\n",
      "2020-12-16 08:17:28 Starting - Launching requested ML instances.........\n",
      "2020-12-16 08:19:00 Starting - Preparing the instances for training......\n",
      "2020-12-16 08:20:19 Downloading - Downloading input data\n",
      "2020-12-16 08:20:19 Training - Downloading the training image...\n",
      "2020-12-16 08:20:33 Training - Training image download completed. Training in progress.\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:40,527 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:40,532 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:41,165 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:41,181 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:41,196 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-16 08:20:41,206 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model\",\n",
      "        \"num_epochs\": 1,\n",
      "        \"data_dir\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2020-12-16-08-17-17-586\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data_dir\":\"/opt/ml/input/data/training\",\"model_dir\":\"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model\",\"num_epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data_dir\":\"/opt/ml/input/data/training\",\"model_dir\":\"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model\",\"num_epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2020-12-16-08-17-17-586\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data_dir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model\",\"--num_epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_DIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --data_dir /opt/ml/input/data/training --model_dir s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model --num_epochs 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34mreading text file\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `layer.add_weight` method instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /opt/ml/code/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:99: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:99: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:101: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:101: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:102: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:102: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:106: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:106: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:107: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:107: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:112: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From train.py:112: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\u001b[0m\n",
      "\u001b[34m0/1352 (epoch 0), train_loss = 4.608, time/batch = 9.226\u001b[0m\n",
      "\u001b[34mmodel saved to s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model/model.ckpt\u001b[0m\n",
      "\u001b[34m1/1352 (epoch 0), train_loss = 4.553, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m2/1352 (epoch 0), train_loss = 4.418, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m3/1352 (epoch 0), train_loss = 4.025, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m4/1352 (epoch 0), train_loss = 3.688, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m5/1352 (epoch 0), train_loss = 3.588, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m6/1352 (epoch 0), train_loss = 3.395, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m7/1352 (epoch 0), train_loss = 3.291, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m8/1352 (epoch 0), train_loss = 3.288, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m9/1352 (epoch 0), train_loss = 3.205, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m10/1352 (epoch 0), train_loss = 3.136, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m11/1352 (epoch 0), train_loss = 3.086, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m12/1352 (epoch 0), train_loss = 3.064, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m13/1352 (epoch 0), train_loss = 3.039, time/batch = 0.270\u001b[0m\n",
      "\u001b[34m14/1352 (epoch 0), train_loss = 3.049, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m15/1352 (epoch 0), train_loss = 3.000, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m16/1352 (epoch 0), train_loss = 3.029, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m17/1352 (epoch 0), train_loss = 3.045, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m18/1352 (epoch 0), train_loss = 3.005, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m19/1352 (epoch 0), train_loss = 2.998, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m20/1352 (epoch 0), train_loss = 2.940, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m21/1352 (epoch 0), train_loss = 2.995, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m22/1352 (epoch 0), train_loss = 2.997, time/batch = 0.164\u001b[0m\n",
      "\u001b[34m23/1352 (epoch 0), train_loss = 2.975, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m24/1352 (epoch 0), train_loss = 3.034, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m25/1352 (epoch 0), train_loss = 2.976, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m26/1352 (epoch 0), train_loss = 2.984, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m27/1352 (epoch 0), train_loss = 3.011, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m28/1352 (epoch 0), train_loss = 2.990, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m29/1352 (epoch 0), train_loss = 2.939, time/batch = 0.235\u001b[0m\n",
      "\u001b[34m30/1352 (epoch 0), train_loss = 2.969, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m31/1352 (epoch 0), train_loss = 2.994, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m32/1352 (epoch 0), train_loss = 2.989, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m33/1352 (epoch 0), train_loss = 2.975, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m34/1352 (epoch 0), train_loss = 2.982, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m35/1352 (epoch 0), train_loss = 2.966, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m36/1352 (epoch 0), train_loss = 2.930, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m37/1352 (epoch 0), train_loss = 2.970, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m38/1352 (epoch 0), train_loss = 2.979, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m39/1352 (epoch 0), train_loss = 2.960, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m40/1352 (epoch 0), train_loss = 2.955, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m41/1352 (epoch 0), train_loss = 2.972, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m42/1352 (epoch 0), train_loss = 2.983, time/batch = 0.163\u001b[0m\n",
      "\u001b[34m43/1352 (epoch 0), train_loss = 3.001, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m44/1352 (epoch 0), train_loss = 3.008, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m45/1352 (epoch 0), train_loss = 2.973, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m46/1352 (epoch 0), train_loss = 2.968, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m47/1352 (epoch 0), train_loss = 2.962, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m48/1352 (epoch 0), train_loss = 2.940, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m49/1352 (epoch 0), train_loss = 2.919, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m50/1352 (epoch 0), train_loss = 3.007, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m51/1352 (epoch 0), train_loss = 2.959, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m52/1352 (epoch 0), train_loss = 2.944, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m53/1352 (epoch 0), train_loss = 3.015, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m54/1352 (epoch 0), train_loss = 3.006, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m55/1352 (epoch 0), train_loss = 2.921, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m56/1352 (epoch 0), train_loss = 2.976, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m57/1352 (epoch 0), train_loss = 2.987, time/batch = 0.164\u001b[0m\n",
      "\u001b[34m58/1352 (epoch 0), train_loss = 2.958, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m59/1352 (epoch 0), train_loss = 2.953, time/batch = 0.208\u001b[0m\n",
      "\u001b[34m60/1352 (epoch 0), train_loss = 2.944, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m61/1352 (epoch 0), train_loss = 2.962, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m62/1352 (epoch 0), train_loss = 2.985, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m63/1352 (epoch 0), train_loss = 2.926, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m64/1352 (epoch 0), train_loss = 2.936, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m65/1352 (epoch 0), train_loss = 2.935, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m66/1352 (epoch 0), train_loss = 2.922, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m67/1352 (epoch 0), train_loss = 2.974, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m68/1352 (epoch 0), train_loss = 2.972, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m69/1352 (epoch 0), train_loss = 2.915, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m70/1352 (epoch 0), train_loss = 2.975, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m71/1352 (epoch 0), train_loss = 2.950, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m72/1352 (epoch 0), train_loss = 2.911, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m73/1352 (epoch 0), train_loss = 2.917, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m74/1352 (epoch 0), train_loss = 2.876, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m75/1352 (epoch 0), train_loss = 2.902, time/batch = 0.163\u001b[0m\n",
      "\u001b[34m76/1352 (epoch 0), train_loss = 2.932, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m77/1352 (epoch 0), train_loss = 2.949, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m78/1352 (epoch 0), train_loss = 2.897, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m79/1352 (epoch 0), train_loss = 2.897, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m80/1352 (epoch 0), train_loss = 2.878, time/batch = 0.164\u001b[0m\n",
      "\u001b[34m81/1352 (epoch 0), train_loss = 2.898, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m82/1352 (epoch 0), train_loss = 2.882, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m83/1352 (epoch 0), train_loss = 2.842, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m84/1352 (epoch 0), train_loss = 2.858, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m85/1352 (epoch 0), train_loss = 2.810, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m86/1352 (epoch 0), train_loss = 2.804, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m87/1352 (epoch 0), train_loss = 2.804, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m88/1352 (epoch 0), train_loss = 2.820, time/batch = 0.272\u001b[0m\n",
      "\u001b[34m89/1352 (epoch 0), train_loss = 2.824, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m90/1352 (epoch 0), train_loss = 2.810, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m91/1352 (epoch 0), train_loss = 2.804, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m92/1352 (epoch 0), train_loss = 2.771, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m93/1352 (epoch 0), train_loss = 2.751, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m94/1352 (epoch 0), train_loss = 2.706, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m95/1352 (epoch 0), train_loss = 2.754, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m96/1352 (epoch 0), train_loss = 2.718, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m97/1352 (epoch 0), train_loss = 2.712, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m98/1352 (epoch 0), train_loss = 2.706, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m99/1352 (epoch 0), train_loss = 2.743, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m100/1352 (epoch 0), train_loss = 2.653, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m101/1352 (epoch 0), train_loss = 2.677, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m102/1352 (epoch 0), train_loss = 2.662, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m103/1352 (epoch 0), train_loss = 2.644, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m104/1352 (epoch 0), train_loss = 2.608, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m105/1352 (epoch 0), train_loss = 2.633, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m106/1352 (epoch 0), train_loss = 2.604, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m107/1352 (epoch 0), train_loss = 2.613, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m108/1352 (epoch 0), train_loss = 2.595, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m109/1352 (epoch 0), train_loss = 2.594, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m110/1352 (epoch 0), train_loss = 2.631, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m111/1352 (epoch 0), train_loss = 2.588, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m112/1352 (epoch 0), train_loss = 2.547, time/batch = 0.238\u001b[0m\n",
      "\u001b[34m113/1352 (epoch 0), train_loss = 2.560, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m114/1352 (epoch 0), train_loss = 2.521, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m115/1352 (epoch 0), train_loss = 2.522, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m116/1352 (epoch 0), train_loss = 2.481, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m117/1352 (epoch 0), train_loss = 2.478, time/batch = 0.220\u001b[0m\n",
      "\u001b[34m118/1352 (epoch 0), train_loss = 2.478, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m119/1352 (epoch 0), train_loss = 2.492, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m120/1352 (epoch 0), train_loss = 2.473, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m121/1352 (epoch 0), train_loss = 2.489, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m122/1352 (epoch 0), train_loss = 2.464, time/batch = 0.196\u001b[0m\n",
      "\u001b[34m123/1352 (epoch 0), train_loss = 2.421, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m124/1352 (epoch 0), train_loss = 2.418, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m125/1352 (epoch 0), train_loss = 2.421, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m126/1352 (epoch 0), train_loss = 2.455, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m127/1352 (epoch 0), train_loss = 2.472, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m128/1352 (epoch 0), train_loss = 2.415, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m129/1352 (epoch 0), train_loss = 2.389, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m130/1352 (epoch 0), train_loss = 2.417, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m131/1352 (epoch 0), train_loss = 2.351, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m132/1352 (epoch 0), train_loss = 2.379, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m133/1352 (epoch 0), train_loss = 2.407, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m134/1352 (epoch 0), train_loss = 2.378, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m135/1352 (epoch 0), train_loss = 2.392, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m136/1352 (epoch 0), train_loss = 2.387, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m137/1352 (epoch 0), train_loss = 2.307, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m138/1352 (epoch 0), train_loss = 2.410, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m139/1352 (epoch 0), train_loss = 2.332, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m140/1352 (epoch 0), train_loss = 2.309, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m141/1352 (epoch 0), train_loss = 2.354, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m142/1352 (epoch 0), train_loss = 2.368, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m143/1352 (epoch 0), train_loss = 2.291, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m144/1352 (epoch 0), train_loss = 2.313, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m145/1352 (epoch 0), train_loss = 2.364, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m146/1352 (epoch 0), train_loss = 2.288, time/batch = 0.217\u001b[0m\n",
      "\u001b[34m147/1352 (epoch 0), train_loss = 2.289, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m148/1352 (epoch 0), train_loss = 2.365, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m149/1352 (epoch 0), train_loss = 2.255, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m150/1352 (epoch 0), train_loss = 2.302, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m151/1352 (epoch 0), train_loss = 2.263, time/batch = 0.200\u001b[0m\n",
      "\u001b[34m152/1352 (epoch 0), train_loss = 2.283, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m153/1352 (epoch 0), train_loss = 2.341, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m154/1352 (epoch 0), train_loss = 2.254, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m155/1352 (epoch 0), train_loss = 2.308, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m156/1352 (epoch 0), train_loss = 2.325, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m157/1352 (epoch 0), train_loss = 2.293, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m158/1352 (epoch 0), train_loss = 2.243, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m159/1352 (epoch 0), train_loss = 2.288, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m160/1352 (epoch 0), train_loss = 2.256, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m161/1352 (epoch 0), train_loss = 2.220, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m162/1352 (epoch 0), train_loss = 2.269, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m163/1352 (epoch 0), train_loss = 2.219, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m164/1352 (epoch 0), train_loss = 2.225, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m165/1352 (epoch 0), train_loss = 2.292, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m166/1352 (epoch 0), train_loss = 2.204, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m167/1352 (epoch 0), train_loss = 2.276, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m168/1352 (epoch 0), train_loss = 2.232, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m169/1352 (epoch 0), train_loss = 2.204, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m170/1352 (epoch 0), train_loss = 2.213, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m171/1352 (epoch 0), train_loss = 2.236, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m172/1352 (epoch 0), train_loss = 2.229, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m173/1352 (epoch 0), train_loss = 2.208, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m174/1352 (epoch 0), train_loss = 2.244, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m175/1352 (epoch 0), train_loss = 2.307, time/batch = 0.235\u001b[0m\n",
      "\u001b[34m176/1352 (epoch 0), train_loss = 2.356, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m177/1352 (epoch 0), train_loss = 2.286, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m178/1352 (epoch 0), train_loss = 2.184, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m179/1352 (epoch 0), train_loss = 2.267, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m180/1352 (epoch 0), train_loss = 2.173, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m181/1352 (epoch 0), train_loss = 2.138, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m182/1352 (epoch 0), train_loss = 2.230, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m183/1352 (epoch 0), train_loss = 2.241, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m184/1352 (epoch 0), train_loss = 2.257, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m185/1352 (epoch 0), train_loss = 2.230, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m186/1352 (epoch 0), train_loss = 2.283, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m187/1352 (epoch 0), train_loss = 2.216, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m188/1352 (epoch 0), train_loss = 2.172, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m189/1352 (epoch 0), train_loss = 2.125, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m190/1352 (epoch 0), train_loss = 2.268, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m191/1352 (epoch 0), train_loss = 2.200, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m192/1352 (epoch 0), train_loss = 2.201, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m193/1352 (epoch 0), train_loss = 2.173, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m194/1352 (epoch 0), train_loss = 2.184, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m195/1352 (epoch 0), train_loss = 2.175, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m196/1352 (epoch 0), train_loss = 2.141, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m197/1352 (epoch 0), train_loss = 2.187, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m198/1352 (epoch 0), train_loss = 2.146, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m199/1352 (epoch 0), train_loss = 2.155, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m200/1352 (epoch 0), train_loss = 2.184, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m201/1352 (epoch 0), train_loss = 2.181, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m202/1352 (epoch 0), train_loss = 2.180, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m203/1352 (epoch 0), train_loss = 2.149, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m204/1352 (epoch 0), train_loss = 2.141, time/batch = 0.235\u001b[0m\n",
      "\u001b[34m205/1352 (epoch 0), train_loss = 2.113, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m206/1352 (epoch 0), train_loss = 2.142, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m207/1352 (epoch 0), train_loss = 2.139, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m208/1352 (epoch 0), train_loss = 2.104, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m209/1352 (epoch 0), train_loss = 2.138, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m210/1352 (epoch 0), train_loss = 2.099, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m211/1352 (epoch 0), train_loss = 2.181, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m212/1352 (epoch 0), train_loss = 2.107, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m213/1352 (epoch 0), train_loss = 2.187, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m214/1352 (epoch 0), train_loss = 2.161, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m215/1352 (epoch 0), train_loss = 2.163, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m216/1352 (epoch 0), train_loss = 2.115, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m217/1352 (epoch 0), train_loss = 2.099, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m218/1352 (epoch 0), train_loss = 2.139, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m219/1352 (epoch 0), train_loss = 2.147, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m220/1352 (epoch 0), train_loss = 2.214, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m221/1352 (epoch 0), train_loss = 2.098, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m222/1352 (epoch 0), train_loss = 2.105, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m223/1352 (epoch 0), train_loss = 2.108, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m224/1352 (epoch 0), train_loss = 2.096, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m225/1352 (epoch 0), train_loss = 2.127, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m226/1352 (epoch 0), train_loss = 2.130, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m227/1352 (epoch 0), train_loss = 2.077, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m228/1352 (epoch 0), train_loss = 2.088, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m229/1352 (epoch 0), train_loss = 2.064, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m230/1352 (epoch 0), train_loss = 2.090, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m231/1352 (epoch 0), train_loss = 2.084, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m232/1352 (epoch 0), train_loss = 2.093, time/batch = 0.220\u001b[0m\n",
      "\u001b[34m233/1352 (epoch 0), train_loss = 2.062, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m234/1352 (epoch 0), train_loss = 2.093, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m235/1352 (epoch 0), train_loss = 2.049, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m236/1352 (epoch 0), train_loss = 2.067, time/batch = 0.197\u001b[0m\n",
      "\u001b[34m237/1352 (epoch 0), train_loss = 2.050, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m238/1352 (epoch 0), train_loss = 2.118, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m239/1352 (epoch 0), train_loss = 2.069, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m240/1352 (epoch 0), train_loss = 2.161, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m241/1352 (epoch 0), train_loss = 2.063, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m242/1352 (epoch 0), train_loss = 2.058, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m243/1352 (epoch 0), train_loss = 2.081, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m244/1352 (epoch 0), train_loss = 2.083, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m245/1352 (epoch 0), train_loss = 2.091, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m246/1352 (epoch 0), train_loss = 2.050, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m247/1352 (epoch 0), train_loss = 2.057, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m248/1352 (epoch 0), train_loss = 2.056, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m249/1352 (epoch 0), train_loss = 2.058, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m250/1352 (epoch 0), train_loss = 2.006, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m251/1352 (epoch 0), train_loss = 2.076, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m252/1352 (epoch 0), train_loss = 2.066, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m253/1352 (epoch 0), train_loss = 2.015, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m254/1352 (epoch 0), train_loss = 2.093, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m255/1352 (epoch 0), train_loss = 2.055, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m256/1352 (epoch 0), train_loss = 2.051, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m257/1352 (epoch 0), train_loss = 2.052, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m258/1352 (epoch 0), train_loss = 2.000, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m259/1352 (epoch 0), train_loss = 2.016, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m260/1352 (epoch 0), train_loss = 1.985, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m261/1352 (epoch 0), train_loss = 2.043, time/batch = 0.211\u001b[0m\n",
      "\u001b[34m262/1352 (epoch 0), train_loss = 2.032, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m263/1352 (epoch 0), train_loss = 2.015, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m264/1352 (epoch 0), train_loss = 1.989, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m265/1352 (epoch 0), train_loss = 2.048, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m266/1352 (epoch 0), train_loss = 1.994, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m267/1352 (epoch 0), train_loss = 1.964, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m268/1352 (epoch 0), train_loss = 2.020, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m269/1352 (epoch 0), train_loss = 2.001, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m270/1352 (epoch 0), train_loss = 2.057, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m271/1352 (epoch 0), train_loss = 1.987, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m272/1352 (epoch 0), train_loss = 2.027, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m273/1352 (epoch 0), train_loss = 2.088, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m274/1352 (epoch 0), train_loss = 1.991, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m275/1352 (epoch 0), train_loss = 2.033, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m276/1352 (epoch 0), train_loss = 2.064, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m277/1352 (epoch 0), train_loss = 1.982, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m278/1352 (epoch 0), train_loss = 1.955, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m279/1352 (epoch 0), train_loss = 2.044, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m280/1352 (epoch 0), train_loss = 2.055, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m281/1352 (epoch 0), train_loss = 2.003, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m282/1352 (epoch 0), train_loss = 1.926, time/batch = 0.259\u001b[0m\n",
      "\u001b[34m283/1352 (epoch 0), train_loss = 2.056, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m284/1352 (epoch 0), train_loss = 2.042, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m285/1352 (epoch 0), train_loss = 2.024, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m286/1352 (epoch 0), train_loss = 1.998, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m287/1352 (epoch 0), train_loss = 2.016, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m288/1352 (epoch 0), train_loss = 2.008, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m289/1352 (epoch 0), train_loss = 2.054, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m290/1352 (epoch 0), train_loss = 2.013, time/batch = 0.213\u001b[0m\n",
      "\u001b[34m291/1352 (epoch 0), train_loss = 1.991, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m292/1352 (epoch 0), train_loss = 1.996, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m293/1352 (epoch 0), train_loss = 1.996, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m294/1352 (epoch 0), train_loss = 2.012, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m295/1352 (epoch 0), train_loss = 2.021, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m296/1352 (epoch 0), train_loss = 1.988, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m297/1352 (epoch 0), train_loss = 1.984, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m298/1352 (epoch 0), train_loss = 1.957, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m299/1352 (epoch 0), train_loss = 2.029, time/batch = 0.194\u001b[0m\n",
      "\u001b[34m300/1352 (epoch 0), train_loss = 1.971, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m301/1352 (epoch 0), train_loss = 2.012, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m302/1352 (epoch 0), train_loss = 2.002, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m303/1352 (epoch 0), train_loss = 1.970, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m304/1352 (epoch 0), train_loss = 2.010, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m305/1352 (epoch 0), train_loss = 2.032, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m306/1352 (epoch 0), train_loss = 1.968, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m307/1352 (epoch 0), train_loss = 1.963, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m308/1352 (epoch 0), train_loss = 2.018, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m309/1352 (epoch 0), train_loss = 1.999, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m310/1352 (epoch 0), train_loss = 1.965, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m311/1352 (epoch 0), train_loss = 2.011, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m312/1352 (epoch 0), train_loss = 1.938, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m313/1352 (epoch 0), train_loss = 1.953, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m314/1352 (epoch 0), train_loss = 2.000, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m315/1352 (epoch 0), train_loss = 2.052, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m316/1352 (epoch 0), train_loss = 2.052, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m317/1352 (epoch 0), train_loss = 1.965, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m318/1352 (epoch 0), train_loss = 2.002, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m319/1352 (epoch 0), train_loss = 2.008, time/batch = 0.225\u001b[0m\n",
      "\u001b[34m320/1352 (epoch 0), train_loss = 2.007, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m321/1352 (epoch 0), train_loss = 1.988, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m322/1352 (epoch 0), train_loss = 1.941, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m323/1352 (epoch 0), train_loss = 1.969, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m324/1352 (epoch 0), train_loss = 1.964, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m325/1352 (epoch 0), train_loss = 1.961, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m326/1352 (epoch 0), train_loss = 1.971, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m327/1352 (epoch 0), train_loss = 1.936, time/batch = 0.204\u001b[0m\n",
      "\u001b[34m328/1352 (epoch 0), train_loss = 1.947, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m329/1352 (epoch 0), train_loss = 2.048, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m330/1352 (epoch 0), train_loss = 1.946, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m331/1352 (epoch 0), train_loss = 1.958, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m332/1352 (epoch 0), train_loss = 2.027, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m333/1352 (epoch 0), train_loss = 1.920, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m334/1352 (epoch 0), train_loss = 1.913, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m335/1352 (epoch 0), train_loss = 1.931, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m336/1352 (epoch 0), train_loss = 1.936, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m337/1352 (epoch 0), train_loss = 1.949, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m338/1352 (epoch 0), train_loss = 2.022, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m339/1352 (epoch 0), train_loss = 1.947, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m340/1352 (epoch 0), train_loss = 1.915, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m341/1352 (epoch 0), train_loss = 1.959, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m342/1352 (epoch 0), train_loss = 1.888, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m343/1352 (epoch 0), train_loss = 1.906, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m344/1352 (epoch 0), train_loss = 1.951, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m345/1352 (epoch 0), train_loss = 2.002, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m346/1352 (epoch 0), train_loss = 1.957, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m347/1352 (epoch 0), train_loss = 1.924, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m348/1352 (epoch 0), train_loss = 2.035, time/batch = 0.200\u001b[0m\n",
      "\u001b[34m349/1352 (epoch 0), train_loss = 1.945, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m350/1352 (epoch 0), train_loss = 1.916, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m351/1352 (epoch 0), train_loss = 1.883, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m352/1352 (epoch 0), train_loss = 1.897, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m353/1352 (epoch 0), train_loss = 1.885, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m354/1352 (epoch 0), train_loss = 1.914, time/batch = 0.283\u001b[0m\n",
      "\u001b[34m355/1352 (epoch 0), train_loss = 1.958, time/batch = 0.202\u001b[0m\n",
      "\u001b[34m356/1352 (epoch 0), train_loss = 1.940, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m357/1352 (epoch 0), train_loss = 1.873, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m358/1352 (epoch 0), train_loss = 1.869, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m359/1352 (epoch 0), train_loss = 2.012, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m360/1352 (epoch 0), train_loss = 1.957, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m361/1352 (epoch 0), train_loss = 1.862, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m362/1352 (epoch 0), train_loss = 1.909, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m363/1352 (epoch 0), train_loss = 1.950, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m364/1352 (epoch 0), train_loss = 1.957, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m365/1352 (epoch 0), train_loss = 1.938, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m366/1352 (epoch 0), train_loss = 1.924, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m367/1352 (epoch 0), train_loss = 1.855, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m368/1352 (epoch 0), train_loss = 1.908, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m369/1352 (epoch 0), train_loss = 1.968, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m370/1352 (epoch 0), train_loss = 1.927, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m371/1352 (epoch 0), train_loss = 1.976, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m372/1352 (epoch 0), train_loss = 1.918, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m373/1352 (epoch 0), train_loss = 1.914, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m374/1352 (epoch 0), train_loss = 1.964, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m375/1352 (epoch 0), train_loss = 1.986, time/batch = 0.213\u001b[0m\n",
      "\u001b[34m376/1352 (epoch 0), train_loss = 1.949, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m377/1352 (epoch 0), train_loss = 1.882, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m378/1352 (epoch 0), train_loss = 1.907, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m379/1352 (epoch 0), train_loss = 1.948, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m380/1352 (epoch 0), train_loss = 1.918, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m381/1352 (epoch 0), train_loss = 1.896, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m382/1352 (epoch 0), train_loss = 1.940, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m383/1352 (epoch 0), train_loss = 1.907, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m384/1352 (epoch 0), train_loss = 2.005, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m385/1352 (epoch 0), train_loss = 1.872, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m386/1352 (epoch 0), train_loss = 1.908, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m387/1352 (epoch 0), train_loss = 1.888, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m388/1352 (epoch 0), train_loss = 1.892, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m389/1352 (epoch 0), train_loss = 1.876, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m390/1352 (epoch 0), train_loss = 1.904, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m391/1352 (epoch 0), train_loss = 1.841, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m392/1352 (epoch 0), train_loss = 1.850, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m393/1352 (epoch 0), train_loss = 1.911, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m394/1352 (epoch 0), train_loss = 1.889, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m395/1352 (epoch 0), train_loss = 1.871, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m396/1352 (epoch 0), train_loss = 1.960, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m397/1352 (epoch 0), train_loss = 1.959, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m398/1352 (epoch 0), train_loss = 1.842, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m399/1352 (epoch 0), train_loss = 1.907, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m400/1352 (epoch 0), train_loss = 1.950, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m401/1352 (epoch 0), train_loss = 1.971, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m402/1352 (epoch 0), train_loss = 1.935, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m403/1352 (epoch 0), train_loss = 1.878, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m404/1352 (epoch 0), train_loss = 1.964, time/batch = 0.338\u001b[0m\n",
      "\u001b[34m405/1352 (epoch 0), train_loss = 1.916, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m406/1352 (epoch 0), train_loss = 2.027, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m407/1352 (epoch 0), train_loss = 1.934, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m408/1352 (epoch 0), train_loss = 1.880, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m409/1352 (epoch 0), train_loss = 1.934, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m410/1352 (epoch 0), train_loss = 1.860, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m411/1352 (epoch 0), train_loss = 1.838, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m412/1352 (epoch 0), train_loss = 1.880, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m413/1352 (epoch 0), train_loss = 1.895, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m414/1352 (epoch 0), train_loss = 1.900, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m415/1352 (epoch 0), train_loss = 1.916, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m416/1352 (epoch 0), train_loss = 1.879, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m417/1352 (epoch 0), train_loss = 1.874, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m418/1352 (epoch 0), train_loss = 1.896, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m419/1352 (epoch 0), train_loss = 1.897, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m420/1352 (epoch 0), train_loss = 1.870, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m421/1352 (epoch 0), train_loss = 1.862, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m422/1352 (epoch 0), train_loss = 1.836, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m423/1352 (epoch 0), train_loss = 1.878, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m424/1352 (epoch 0), train_loss = 1.917, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m425/1352 (epoch 0), train_loss = 1.880, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m426/1352 (epoch 0), train_loss = 1.940, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m427/1352 (epoch 0), train_loss = 1.877, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m428/1352 (epoch 0), train_loss = 1.801, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m429/1352 (epoch 0), train_loss = 1.857, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m430/1352 (epoch 0), train_loss = 1.933, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m431/1352 (epoch 0), train_loss = 1.878, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m432/1352 (epoch 0), train_loss = 1.848, time/batch = 0.255\u001b[0m\n",
      "\u001b[34m433/1352 (epoch 0), train_loss = 1.856, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m434/1352 (epoch 0), train_loss = 1.868, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m435/1352 (epoch 0), train_loss = 1.943, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m436/1352 (epoch 0), train_loss = 1.896, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m437/1352 (epoch 0), train_loss = 1.876, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m438/1352 (epoch 0), train_loss = 1.896, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m439/1352 (epoch 0), train_loss = 1.901, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m440/1352 (epoch 0), train_loss = 1.846, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m441/1352 (epoch 0), train_loss = 1.889, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m442/1352 (epoch 0), train_loss = 1.846, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m443/1352 (epoch 0), train_loss = 1.836, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m444/1352 (epoch 0), train_loss = 1.855, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m445/1352 (epoch 0), train_loss = 1.895, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m446/1352 (epoch 0), train_loss = 1.834, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m447/1352 (epoch 0), train_loss = 1.819, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m448/1352 (epoch 0), train_loss = 1.841, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m449/1352 (epoch 0), train_loss = 1.822, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m450/1352 (epoch 0), train_loss = 1.811, time/batch = 0.243\u001b[0m\n",
      "\u001b[34m451/1352 (epoch 0), train_loss = 1.972, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m452/1352 (epoch 0), train_loss = 1.899, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m453/1352 (epoch 0), train_loss = 1.795, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m454/1352 (epoch 0), train_loss = 1.818, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m455/1352 (epoch 0), train_loss = 1.867, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m456/1352 (epoch 0), train_loss = 1.869, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m457/1352 (epoch 0), train_loss = 1.847, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m458/1352 (epoch 0), train_loss = 1.881, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m459/1352 (epoch 0), train_loss = 1.883, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m460/1352 (epoch 0), train_loss = 1.860, time/batch = 0.197\u001b[0m\n",
      "\u001b[34m461/1352 (epoch 0), train_loss = 1.774, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m462/1352 (epoch 0), train_loss = 1.854, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m463/1352 (epoch 0), train_loss = 1.863, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m464/1352 (epoch 0), train_loss = 1.827, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m465/1352 (epoch 0), train_loss = 1.820, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m466/1352 (epoch 0), train_loss = 1.872, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m467/1352 (epoch 0), train_loss = 1.912, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m468/1352 (epoch 0), train_loss = 1.873, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m469/1352 (epoch 0), train_loss = 1.845, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m470/1352 (epoch 0), train_loss = 1.844, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m471/1352 (epoch 0), train_loss = 1.824, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m472/1352 (epoch 0), train_loss = 1.813, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m473/1352 (epoch 0), train_loss = 1.831, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m474/1352 (epoch 0), train_loss = 1.844, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m475/1352 (epoch 0), train_loss = 1.804, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m476/1352 (epoch 0), train_loss = 1.817, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m477/1352 (epoch 0), train_loss = 1.837, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m478/1352 (epoch 0), train_loss = 1.862, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m479/1352 (epoch 0), train_loss = 1.857, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m480/1352 (epoch 0), train_loss = 1.767, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m481/1352 (epoch 0), train_loss = 1.814, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m482/1352 (epoch 0), train_loss = 1.814, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m483/1352 (epoch 0), train_loss = 1.802, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m484/1352 (epoch 0), train_loss = 1.783, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m485/1352 (epoch 0), train_loss = 1.865, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m486/1352 (epoch 0), train_loss = 1.812, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m487/1352 (epoch 0), train_loss = 1.811, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m488/1352 (epoch 0), train_loss = 1.860, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m489/1352 (epoch 0), train_loss = 1.857, time/batch = 0.220\u001b[0m\n",
      "\u001b[34m490/1352 (epoch 0), train_loss = 1.762, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m491/1352 (epoch 0), train_loss = 1.820, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m492/1352 (epoch 0), train_loss = 1.836, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m493/1352 (epoch 0), train_loss = 1.781, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m494/1352 (epoch 0), train_loss = 1.839, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m495/1352 (epoch 0), train_loss = 1.820, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m496/1352 (epoch 0), train_loss = 1.788, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m497/1352 (epoch 0), train_loss = 1.802, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m498/1352 (epoch 0), train_loss = 1.828, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m499/1352 (epoch 0), train_loss = 1.791, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m500/1352 (epoch 0), train_loss = 1.813, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m501/1352 (epoch 0), train_loss = 1.739, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m502/1352 (epoch 0), train_loss = 1.832, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m503/1352 (epoch 0), train_loss = 1.816, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m504/1352 (epoch 0), train_loss = 1.832, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m505/1352 (epoch 0), train_loss = 1.846, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m506/1352 (epoch 0), train_loss = 1.818, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m507/1352 (epoch 0), train_loss = 1.817, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m508/1352 (epoch 0), train_loss = 1.783, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m509/1352 (epoch 0), train_loss = 1.785, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m510/1352 (epoch 0), train_loss = 1.887, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m511/1352 (epoch 0), train_loss = 1.843, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m512/1352 (epoch 0), train_loss = 1.818, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m513/1352 (epoch 0), train_loss = 1.778, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m514/1352 (epoch 0), train_loss = 1.813, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m515/1352 (epoch 0), train_loss = 1.818, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m516/1352 (epoch 0), train_loss = 1.821, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m517/1352 (epoch 0), train_loss = 1.836, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m518/1352 (epoch 0), train_loss = 1.836, time/batch = 0.217\u001b[0m\n",
      "\u001b[34m519/1352 (epoch 0), train_loss = 1.722, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m520/1352 (epoch 0), train_loss = 1.796, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m521/1352 (epoch 0), train_loss = 1.814, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m522/1352 (epoch 0), train_loss = 1.835, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m523/1352 (epoch 0), train_loss = 1.787, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m524/1352 (epoch 0), train_loss = 1.791, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m525/1352 (epoch 0), train_loss = 1.794, time/batch = 0.197\u001b[0m\n",
      "\u001b[34m526/1352 (epoch 0), train_loss = 1.774, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m527/1352 (epoch 0), train_loss = 1.802, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m528/1352 (epoch 0), train_loss = 1.847, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m529/1352 (epoch 0), train_loss = 1.774, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m530/1352 (epoch 0), train_loss = 1.831, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m531/1352 (epoch 0), train_loss = 1.856, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m532/1352 (epoch 0), train_loss = 1.785, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m533/1352 (epoch 0), train_loss = 1.764, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m534/1352 (epoch 0), train_loss = 1.764, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m535/1352 (epoch 0), train_loss = 1.761, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m536/1352 (epoch 0), train_loss = 1.808, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m537/1352 (epoch 0), train_loss = 1.781, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m538/1352 (epoch 0), train_loss = 1.787, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m539/1352 (epoch 0), train_loss = 1.812, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m540/1352 (epoch 0), train_loss = 1.781, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m541/1352 (epoch 0), train_loss = 1.747, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m542/1352 (epoch 0), train_loss = 1.790, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m543/1352 (epoch 0), train_loss = 1.694, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m544/1352 (epoch 0), train_loss = 1.785, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m545/1352 (epoch 0), train_loss = 1.771, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m546/1352 (epoch 0), train_loss = 1.817, time/batch = 0.223\u001b[0m\n",
      "\u001b[34m547/1352 (epoch 0), train_loss = 1.755, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m548/1352 (epoch 0), train_loss = 1.892, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m549/1352 (epoch 0), train_loss = 1.824, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m550/1352 (epoch 0), train_loss = 1.785, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m551/1352 (epoch 0), train_loss = 1.805, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m552/1352 (epoch 0), train_loss = 1.727, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m553/1352 (epoch 0), train_loss = 1.793, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m554/1352 (epoch 0), train_loss = 1.800, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m555/1352 (epoch 0), train_loss = 1.764, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m556/1352 (epoch 0), train_loss = 1.783, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m557/1352 (epoch 0), train_loss = 1.759, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m558/1352 (epoch 0), train_loss = 1.745, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m559/1352 (epoch 0), train_loss = 1.761, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m560/1352 (epoch 0), train_loss = 1.757, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m561/1352 (epoch 0), train_loss = 1.736, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m562/1352 (epoch 0), train_loss = 1.789, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m563/1352 (epoch 0), train_loss = 1.757, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m564/1352 (epoch 0), train_loss = 1.767, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m565/1352 (epoch 0), train_loss = 1.767, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m566/1352 (epoch 0), train_loss = 1.794, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m567/1352 (epoch 0), train_loss = 1.849, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m568/1352 (epoch 0), train_loss = 1.782, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m569/1352 (epoch 0), train_loss = 1.748, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m570/1352 (epoch 0), train_loss = 1.740, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m571/1352 (epoch 0), train_loss = 1.767, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m572/1352 (epoch 0), train_loss = 1.744, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m573/1352 (epoch 0), train_loss = 1.792, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m574/1352 (epoch 0), train_loss = 1.773, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m575/1352 (epoch 0), train_loss = 1.749, time/batch = 0.209\u001b[0m\n",
      "\u001b[34m576/1352 (epoch 0), train_loss = 1.802, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m577/1352 (epoch 0), train_loss = 1.792, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m578/1352 (epoch 0), train_loss = 1.835, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m579/1352 (epoch 0), train_loss = 1.777, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m580/1352 (epoch 0), train_loss = 1.732, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m581/1352 (epoch 0), train_loss = 1.783, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m582/1352 (epoch 0), train_loss = 1.728, time/batch = 0.200\u001b[0m\n",
      "\u001b[34m583/1352 (epoch 0), train_loss = 1.739, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m584/1352 (epoch 0), train_loss = 1.774, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m585/1352 (epoch 0), train_loss = 1.794, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m586/1352 (epoch 0), train_loss = 1.732, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m587/1352 (epoch 0), train_loss = 1.822, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m588/1352 (epoch 0), train_loss = 1.727, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m589/1352 (epoch 0), train_loss = 1.741, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m590/1352 (epoch 0), train_loss = 1.798, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m591/1352 (epoch 0), train_loss = 1.724, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m592/1352 (epoch 0), train_loss = 1.785, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m593/1352 (epoch 0), train_loss = 1.738, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m594/1352 (epoch 0), train_loss = 1.717, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m595/1352 (epoch 0), train_loss = 1.771, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m596/1352 (epoch 0), train_loss = 1.743, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m597/1352 (epoch 0), train_loss = 1.721, time/batch = 0.203\u001b[0m\n",
      "\u001b[34m598/1352 (epoch 0), train_loss = 1.704, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m599/1352 (epoch 0), train_loss = 1.734, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m600/1352 (epoch 0), train_loss = 1.790, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m601/1352 (epoch 0), train_loss = 1.737, time/batch = 0.166\u001b[0m\n",
      "\u001b[34m602/1352 (epoch 0), train_loss = 1.753, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m603/1352 (epoch 0), train_loss = 1.752, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m604/1352 (epoch 0), train_loss = 1.786, time/batch = 0.213\u001b[0m\n",
      "\u001b[34m605/1352 (epoch 0), train_loss = 1.730, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m606/1352 (epoch 0), train_loss = 1.797, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m607/1352 (epoch 0), train_loss = 1.722, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m608/1352 (epoch 0), train_loss = 1.771, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m609/1352 (epoch 0), train_loss = 1.752, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m610/1352 (epoch 0), train_loss = 1.695, time/batch = 0.202\u001b[0m\n",
      "\u001b[34m611/1352 (epoch 0), train_loss = 1.824, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m612/1352 (epoch 0), train_loss = 1.770, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m613/1352 (epoch 0), train_loss = 1.704, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m614/1352 (epoch 0), train_loss = 1.730, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m615/1352 (epoch 0), train_loss = 1.824, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m616/1352 (epoch 0), train_loss = 1.768, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m617/1352 (epoch 0), train_loss = 1.808, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m618/1352 (epoch 0), train_loss = 1.728, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m619/1352 (epoch 0), train_loss = 1.714, time/batch = 0.224\u001b[0m\n",
      "\u001b[34m620/1352 (epoch 0), train_loss = 1.731, time/batch = 0.214\u001b[0m\n",
      "\u001b[34m621/1352 (epoch 0), train_loss = 1.736, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m622/1352 (epoch 0), train_loss = 1.757, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m623/1352 (epoch 0), train_loss = 1.731, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m624/1352 (epoch 0), train_loss = 1.735, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m625/1352 (epoch 0), train_loss = 1.745, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m626/1352 (epoch 0), train_loss = 1.740, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m627/1352 (epoch 0), train_loss = 1.816, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m628/1352 (epoch 0), train_loss = 1.778, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m629/1352 (epoch 0), train_loss = 1.715, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m630/1352 (epoch 0), train_loss = 1.773, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m631/1352 (epoch 0), train_loss = 1.784, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m632/1352 (epoch 0), train_loss = 1.756, time/batch = 0.215\u001b[0m\n",
      "\u001b[34m633/1352 (epoch 0), train_loss = 1.849, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m634/1352 (epoch 0), train_loss = 1.821, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m635/1352 (epoch 0), train_loss = 1.770, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m636/1352 (epoch 0), train_loss = 1.803, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m637/1352 (epoch 0), train_loss = 1.734, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m638/1352 (epoch 0), train_loss = 1.723, time/batch = 0.196\u001b[0m\n",
      "\u001b[34m639/1352 (epoch 0), train_loss = 1.706, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m640/1352 (epoch 0), train_loss = 1.761, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m641/1352 (epoch 0), train_loss = 1.744, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m642/1352 (epoch 0), train_loss = 1.749, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m643/1352 (epoch 0), train_loss = 1.789, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m644/1352 (epoch 0), train_loss = 1.767, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m645/1352 (epoch 0), train_loss = 1.731, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m646/1352 (epoch 0), train_loss = 1.718, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m647/1352 (epoch 0), train_loss = 1.729, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m648/1352 (epoch 0), train_loss = 1.757, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m649/1352 (epoch 0), train_loss = 1.762, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m650/1352 (epoch 0), train_loss = 1.807, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m651/1352 (epoch 0), train_loss = 1.761, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m652/1352 (epoch 0), train_loss = 1.710, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m653/1352 (epoch 0), train_loss = 1.737, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m654/1352 (epoch 0), train_loss = 1.723, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m655/1352 (epoch 0), train_loss = 1.716, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m656/1352 (epoch 0), train_loss = 1.713, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m657/1352 (epoch 0), train_loss = 1.774, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m658/1352 (epoch 0), train_loss = 1.760, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m659/1352 (epoch 0), train_loss = 1.670, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m660/1352 (epoch 0), train_loss = 1.741, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m661/1352 (epoch 0), train_loss = 1.726, time/batch = 0.224\u001b[0m\n",
      "\u001b[34m662/1352 (epoch 0), train_loss = 1.672, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m663/1352 (epoch 0), train_loss = 1.678, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m664/1352 (epoch 0), train_loss = 1.738, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m665/1352 (epoch 0), train_loss = 1.687, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m666/1352 (epoch 0), train_loss = 1.723, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m667/1352 (epoch 0), train_loss = 1.724, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m668/1352 (epoch 0), train_loss = 1.741, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m669/1352 (epoch 0), train_loss = 1.645, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m670/1352 (epoch 0), train_loss = 1.667, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m671/1352 (epoch 0), train_loss = 1.731, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m672/1352 (epoch 0), train_loss = 1.660, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m673/1352 (epoch 0), train_loss = 1.748, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m674/1352 (epoch 0), train_loss = 1.749, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m675/1352 (epoch 0), train_loss = 1.696, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m676/1352 (epoch 0), train_loss = 1.711, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m677/1352 (epoch 0), train_loss = 1.744, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m678/1352 (epoch 0), train_loss = 1.715, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m679/1352 (epoch 0), train_loss = 1.717, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m680/1352 (epoch 0), train_loss = 1.705, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m681/1352 (epoch 0), train_loss = 1.757, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m682/1352 (epoch 0), train_loss = 1.691, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m683/1352 (epoch 0), train_loss = 1.734, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m684/1352 (epoch 0), train_loss = 1.728, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m685/1352 (epoch 0), train_loss = 1.740, time/batch = 0.255\u001b[0m\n",
      "\u001b[34m686/1352 (epoch 0), train_loss = 1.740, time/batch = 0.200\u001b[0m\n",
      "\u001b[34m687/1352 (epoch 0), train_loss = 1.717, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m688/1352 (epoch 0), train_loss = 1.708, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m689/1352 (epoch 0), train_loss = 1.736, time/batch = 0.230\u001b[0m\n",
      "\u001b[34m690/1352 (epoch 0), train_loss = 1.718, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m691/1352 (epoch 0), train_loss = 1.710, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m692/1352 (epoch 0), train_loss = 1.718, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m693/1352 (epoch 0), train_loss = 1.735, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m694/1352 (epoch 0), train_loss = 1.684, time/batch = 0.207\u001b[0m\n",
      "\u001b[34m695/1352 (epoch 0), train_loss = 1.676, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m696/1352 (epoch 0), train_loss = 1.712, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m697/1352 (epoch 0), train_loss = 1.748, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m698/1352 (epoch 0), train_loss = 1.686, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m699/1352 (epoch 0), train_loss = 1.686, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m700/1352 (epoch 0), train_loss = 1.734, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m701/1352 (epoch 0), train_loss = 1.744, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m702/1352 (epoch 0), train_loss = 1.701, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m703/1352 (epoch 0), train_loss = 1.687, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m704/1352 (epoch 0), train_loss = 1.674, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m705/1352 (epoch 0), train_loss = 1.703, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m706/1352 (epoch 0), train_loss = 1.706, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m707/1352 (epoch 0), train_loss = 1.721, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m708/1352 (epoch 0), train_loss = 1.733, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m709/1352 (epoch 0), train_loss = 1.679, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m710/1352 (epoch 0), train_loss = 1.689, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m711/1352 (epoch 0), train_loss = 1.673, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m712/1352 (epoch 0), train_loss = 1.706, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m713/1352 (epoch 0), train_loss = 1.747, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m714/1352 (epoch 0), train_loss = 1.668, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m715/1352 (epoch 0), train_loss = 1.753, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m716/1352 (epoch 0), train_loss = 1.643, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m717/1352 (epoch 0), train_loss = 1.705, time/batch = 0.245\u001b[0m\n",
      "\u001b[34m718/1352 (epoch 0), train_loss = 1.675, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m719/1352 (epoch 0), train_loss = 1.647, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m720/1352 (epoch 0), train_loss = 1.646, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m721/1352 (epoch 0), train_loss = 1.707, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m722/1352 (epoch 0), train_loss = 1.688, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m723/1352 (epoch 0), train_loss = 1.727, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m724/1352 (epoch 0), train_loss = 1.691, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m725/1352 (epoch 0), train_loss = 1.758, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m726/1352 (epoch 0), train_loss = 1.773, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m727/1352 (epoch 0), train_loss = 1.738, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m728/1352 (epoch 0), train_loss = 1.700, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m729/1352 (epoch 0), train_loss = 1.684, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m730/1352 (epoch 0), train_loss = 1.697, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m731/1352 (epoch 0), train_loss = 1.700, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m732/1352 (epoch 0), train_loss = 1.665, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m733/1352 (epoch 0), train_loss = 1.668, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m734/1352 (epoch 0), train_loss = 1.695, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m735/1352 (epoch 0), train_loss = 1.649, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m736/1352 (epoch 0), train_loss = 1.724, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m737/1352 (epoch 0), train_loss = 1.679, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m738/1352 (epoch 0), train_loss = 1.641, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m739/1352 (epoch 0), train_loss = 1.730, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m740/1352 (epoch 0), train_loss = 1.625, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m741/1352 (epoch 0), train_loss = 1.667, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m742/1352 (epoch 0), train_loss = 1.621, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m743/1352 (epoch 0), train_loss = 1.684, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m744/1352 (epoch 0), train_loss = 1.675, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m745/1352 (epoch 0), train_loss = 1.717, time/batch = 0.203\u001b[0m\n",
      "\u001b[34m746/1352 (epoch 0), train_loss = 1.667, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m747/1352 (epoch 0), train_loss = 1.688, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m748/1352 (epoch 0), train_loss = 1.705, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m749/1352 (epoch 0), train_loss = 1.674, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m750/1352 (epoch 0), train_loss = 1.699, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m751/1352 (epoch 0), train_loss = 1.634, time/batch = 0.200\u001b[0m\n",
      "\u001b[34m752/1352 (epoch 0), train_loss = 1.660, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m753/1352 (epoch 0), train_loss = 1.701, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m754/1352 (epoch 0), train_loss = 1.610, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m755/1352 (epoch 0), train_loss = 1.667, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m756/1352 (epoch 0), train_loss = 1.750, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m757/1352 (epoch 0), train_loss = 1.715, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m758/1352 (epoch 0), train_loss = 1.644, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m759/1352 (epoch 0), train_loss = 1.620, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m760/1352 (epoch 0), train_loss = 1.745, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m761/1352 (epoch 0), train_loss = 1.678, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m762/1352 (epoch 0), train_loss = 1.647, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m763/1352 (epoch 0), train_loss = 1.736, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m764/1352 (epoch 0), train_loss = 1.807, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m765/1352 (epoch 0), train_loss = 1.622, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m766/1352 (epoch 0), train_loss = 1.701, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m767/1352 (epoch 0), train_loss = 1.673, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m768/1352 (epoch 0), train_loss = 1.630, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m769/1352 (epoch 0), train_loss = 1.774, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m770/1352 (epoch 0), train_loss = 1.671, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m771/1352 (epoch 0), train_loss = 1.696, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m772/1352 (epoch 0), train_loss = 1.725, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m773/1352 (epoch 0), train_loss = 1.703, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m774/1352 (epoch 0), train_loss = 1.745, time/batch = 0.224\u001b[0m\n",
      "\u001b[34m775/1352 (epoch 0), train_loss = 1.742, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m776/1352 (epoch 0), train_loss = 1.683, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m777/1352 (epoch 0), train_loss = 1.671, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m778/1352 (epoch 0), train_loss = 1.689, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m779/1352 (epoch 0), train_loss = 1.725, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m780/1352 (epoch 0), train_loss = 1.711, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m781/1352 (epoch 0), train_loss = 1.790, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m782/1352 (epoch 0), train_loss = 1.658, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m783/1352 (epoch 0), train_loss = 1.686, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m784/1352 (epoch 0), train_loss = 1.675, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m785/1352 (epoch 0), train_loss = 1.719, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m786/1352 (epoch 0), train_loss = 1.727, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m787/1352 (epoch 0), train_loss = 1.723, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m788/1352 (epoch 0), train_loss = 1.686, time/batch = 0.257\u001b[0m\n",
      "\u001b[34m789/1352 (epoch 0), train_loss = 1.673, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m790/1352 (epoch 0), train_loss = 1.668, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m791/1352 (epoch 0), train_loss = 1.659, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m792/1352 (epoch 0), train_loss = 1.671, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m793/1352 (epoch 0), train_loss = 1.719, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m794/1352 (epoch 0), train_loss = 1.652, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m795/1352 (epoch 0), train_loss = 1.680, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m796/1352 (epoch 0), train_loss = 1.703, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m797/1352 (epoch 0), train_loss = 1.706, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m798/1352 (epoch 0), train_loss = 1.685, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m799/1352 (epoch 0), train_loss = 1.669, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m800/1352 (epoch 0), train_loss = 1.712, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m801/1352 (epoch 0), train_loss = 1.752, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m802/1352 (epoch 0), train_loss = 1.712, time/batch = 0.227\u001b[0m\n",
      "\u001b[34m803/1352 (epoch 0), train_loss = 1.711, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m804/1352 (epoch 0), train_loss = 1.650, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m805/1352 (epoch 0), train_loss = 1.638, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m806/1352 (epoch 0), train_loss = 1.736, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m807/1352 (epoch 0), train_loss = 1.656, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m808/1352 (epoch 0), train_loss = 1.601, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m809/1352 (epoch 0), train_loss = 1.690, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m810/1352 (epoch 0), train_loss = 1.651, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m811/1352 (epoch 0), train_loss = 1.754, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m812/1352 (epoch 0), train_loss = 1.663, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m813/1352 (epoch 0), train_loss = 1.711, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m814/1352 (epoch 0), train_loss = 1.675, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m815/1352 (epoch 0), train_loss = 1.630, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m816/1352 (epoch 0), train_loss = 1.708, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m817/1352 (epoch 0), train_loss = 1.615, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m818/1352 (epoch 0), train_loss = 1.637, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m819/1352 (epoch 0), train_loss = 1.650, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m820/1352 (epoch 0), train_loss = 1.596, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m821/1352 (epoch 0), train_loss = 1.672, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m822/1352 (epoch 0), train_loss = 1.664, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m823/1352 (epoch 0), train_loss = 1.693, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m824/1352 (epoch 0), train_loss = 1.635, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m825/1352 (epoch 0), train_loss = 1.697, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m826/1352 (epoch 0), train_loss = 1.796, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m827/1352 (epoch 0), train_loss = 1.655, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m828/1352 (epoch 0), train_loss = 1.680, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m829/1352 (epoch 0), train_loss = 1.647, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m830/1352 (epoch 0), train_loss = 1.705, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m831/1352 (epoch 0), train_loss = 1.667, time/batch = 0.233\u001b[0m\n",
      "\u001b[34m832/1352 (epoch 0), train_loss = 1.698, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m833/1352 (epoch 0), train_loss = 1.688, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m834/1352 (epoch 0), train_loss = 1.617, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m835/1352 (epoch 0), train_loss = 1.706, time/batch = 0.195\u001b[0m\n",
      "\u001b[34m836/1352 (epoch 0), train_loss = 1.597, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m837/1352 (epoch 0), train_loss = 1.716, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m838/1352 (epoch 0), train_loss = 1.568, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m839/1352 (epoch 0), train_loss = 1.684, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m840/1352 (epoch 0), train_loss = 1.712, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m841/1352 (epoch 0), train_loss = 1.673, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m842/1352 (epoch 0), train_loss = 1.646, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m843/1352 (epoch 0), train_loss = 1.711, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m844/1352 (epoch 0), train_loss = 1.664, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m845/1352 (epoch 0), train_loss = 1.652, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m846/1352 (epoch 0), train_loss = 1.681, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m847/1352 (epoch 0), train_loss = 1.614, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m848/1352 (epoch 0), train_loss = 1.694, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m849/1352 (epoch 0), train_loss = 1.673, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m850/1352 (epoch 0), train_loss = 1.657, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m851/1352 (epoch 0), train_loss = 1.681, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m852/1352 (epoch 0), train_loss = 1.699, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m853/1352 (epoch 0), train_loss = 1.667, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m854/1352 (epoch 0), train_loss = 1.653, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m855/1352 (epoch 0), train_loss = 1.703, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m856/1352 (epoch 0), train_loss = 1.671, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m857/1352 (epoch 0), train_loss = 1.657, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m858/1352 (epoch 0), train_loss = 1.693, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m859/1352 (epoch 0), train_loss = 1.687, time/batch = 0.214\u001b[0m\n",
      "\u001b[34m860/1352 (epoch 0), train_loss = 1.682, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m861/1352 (epoch 0), train_loss = 1.726, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m862/1352 (epoch 0), train_loss = 1.723, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m863/1352 (epoch 0), train_loss = 1.714, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m864/1352 (epoch 0), train_loss = 1.646, time/batch = 0.195\u001b[0m\n",
      "\u001b[34m865/1352 (epoch 0), train_loss = 1.742, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m866/1352 (epoch 0), train_loss = 1.711, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m867/1352 (epoch 0), train_loss = 1.599, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m868/1352 (epoch 0), train_loss = 1.650, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m869/1352 (epoch 0), train_loss = 1.658, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m870/1352 (epoch 0), train_loss = 1.629, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m871/1352 (epoch 0), train_loss = 1.661, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m872/1352 (epoch 0), train_loss = 1.649, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m873/1352 (epoch 0), train_loss = 1.664, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m874/1352 (epoch 0), train_loss = 1.633, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m875/1352 (epoch 0), train_loss = 1.712, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m876/1352 (epoch 0), train_loss = 1.646, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m877/1352 (epoch 0), train_loss = 1.578, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m878/1352 (epoch 0), train_loss = 1.658, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m879/1352 (epoch 0), train_loss = 1.607, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m880/1352 (epoch 0), train_loss = 1.597, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m881/1352 (epoch 0), train_loss = 1.619, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m882/1352 (epoch 0), train_loss = 1.605, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m883/1352 (epoch 0), train_loss = 1.639, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m884/1352 (epoch 0), train_loss = 1.624, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m885/1352 (epoch 0), train_loss = 1.671, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m886/1352 (epoch 0), train_loss = 1.632, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m887/1352 (epoch 0), train_loss = 1.568, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m888/1352 (epoch 0), train_loss = 1.742, time/batch = 0.214\u001b[0m\n",
      "\u001b[34m889/1352 (epoch 0), train_loss = 1.664, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m890/1352 (epoch 0), train_loss = 1.719, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m891/1352 (epoch 0), train_loss = 1.689, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m892/1352 (epoch 0), train_loss = 1.652, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m893/1352 (epoch 0), train_loss = 1.661, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m894/1352 (epoch 0), train_loss = 1.657, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m895/1352 (epoch 0), train_loss = 1.647, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m896/1352 (epoch 0), train_loss = 1.662, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m897/1352 (epoch 0), train_loss = 1.672, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m898/1352 (epoch 0), train_loss = 1.685, time/batch = 0.196\u001b[0m\n",
      "\u001b[34m899/1352 (epoch 0), train_loss = 1.692, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m900/1352 (epoch 0), train_loss = 1.640, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m901/1352 (epoch 0), train_loss = 1.672, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m902/1352 (epoch 0), train_loss = 1.696, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m903/1352 (epoch 0), train_loss = 1.668, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m904/1352 (epoch 0), train_loss = 1.638, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m905/1352 (epoch 0), train_loss = 1.695, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m906/1352 (epoch 0), train_loss = 1.638, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m907/1352 (epoch 0), train_loss = 1.637, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m908/1352 (epoch 0), train_loss = 1.682, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m909/1352 (epoch 0), train_loss = 1.650, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m910/1352 (epoch 0), train_loss = 1.637, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m911/1352 (epoch 0), train_loss = 1.680, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m912/1352 (epoch 0), train_loss = 1.628, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m913/1352 (epoch 0), train_loss = 1.636, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m914/1352 (epoch 0), train_loss = 1.672, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m915/1352 (epoch 0), train_loss = 1.595, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m916/1352 (epoch 0), train_loss = 1.629, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m917/1352 (epoch 0), train_loss = 1.599, time/batch = 0.229\u001b[0m\n",
      "\u001b[34m918/1352 (epoch 0), train_loss = 1.613, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m919/1352 (epoch 0), train_loss = 1.641, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m920/1352 (epoch 0), train_loss = 1.660, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m921/1352 (epoch 0), train_loss = 1.612, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m922/1352 (epoch 0), train_loss = 1.650, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m923/1352 (epoch 0), train_loss = 1.609, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m924/1352 (epoch 0), train_loss = 1.584, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m925/1352 (epoch 0), train_loss = 1.689, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m926/1352 (epoch 0), train_loss = 1.626, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m927/1352 (epoch 0), train_loss = 1.652, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m928/1352 (epoch 0), train_loss = 1.616, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m929/1352 (epoch 0), train_loss = 1.639, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m930/1352 (epoch 0), train_loss = 1.586, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m931/1352 (epoch 0), train_loss = 1.670, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m932/1352 (epoch 0), train_loss = 1.651, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m933/1352 (epoch 0), train_loss = 1.582, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m934/1352 (epoch 0), train_loss = 1.609, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m935/1352 (epoch 0), train_loss = 1.636, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m936/1352 (epoch 0), train_loss = 1.653, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m937/1352 (epoch 0), train_loss = 1.592, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m938/1352 (epoch 0), train_loss = 1.571, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m939/1352 (epoch 0), train_loss = 1.608, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m940/1352 (epoch 0), train_loss = 1.628, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m941/1352 (epoch 0), train_loss = 1.601, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m942/1352 (epoch 0), train_loss = 1.540, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m943/1352 (epoch 0), train_loss = 1.667, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m944/1352 (epoch 0), train_loss = 1.610, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m945/1352 (epoch 0), train_loss = 1.706, time/batch = 0.217\u001b[0m\n",
      "\u001b[34m946/1352 (epoch 0), train_loss = 1.629, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m947/1352 (epoch 0), train_loss = 1.590, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m948/1352 (epoch 0), train_loss = 1.654, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m949/1352 (epoch 0), train_loss = 1.634, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m950/1352 (epoch 0), train_loss = 1.668, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m951/1352 (epoch 0), train_loss = 1.597, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m952/1352 (epoch 0), train_loss = 1.681, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m953/1352 (epoch 0), train_loss = 1.587, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m954/1352 (epoch 0), train_loss = 1.567, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m955/1352 (epoch 0), train_loss = 1.644, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m956/1352 (epoch 0), train_loss = 1.649, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m957/1352 (epoch 0), train_loss = 1.663, time/batch = 0.266\u001b[0m\n",
      "\u001b[34m958/1352 (epoch 0), train_loss = 1.643, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m959/1352 (epoch 0), train_loss = 1.638, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m960/1352 (epoch 0), train_loss = 1.617, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m961/1352 (epoch 0), train_loss = 1.539, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m962/1352 (epoch 0), train_loss = 1.592, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m963/1352 (epoch 0), train_loss = 1.584, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m964/1352 (epoch 0), train_loss = 1.553, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m965/1352 (epoch 0), train_loss = 1.612, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m966/1352 (epoch 0), train_loss = 1.641, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m967/1352 (epoch 0), train_loss = 1.595, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m968/1352 (epoch 0), train_loss = 1.593, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m969/1352 (epoch 0), train_loss = 1.551, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m970/1352 (epoch 0), train_loss = 1.641, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m971/1352 (epoch 0), train_loss = 1.655, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m972/1352 (epoch 0), train_loss = 1.595, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m973/1352 (epoch 0), train_loss = 1.627, time/batch = 0.211\u001b[0m\n",
      "\u001b[34m974/1352 (epoch 0), train_loss = 1.660, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m975/1352 (epoch 0), train_loss = 1.544, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m976/1352 (epoch 0), train_loss = 1.548, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m977/1352 (epoch 0), train_loss = 1.597, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m978/1352 (epoch 0), train_loss = 1.675, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m979/1352 (epoch 0), train_loss = 1.623, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m980/1352 (epoch 0), train_loss = 1.672, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m981/1352 (epoch 0), train_loss = 1.593, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m982/1352 (epoch 0), train_loss = 1.568, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m983/1352 (epoch 0), train_loss = 1.621, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m984/1352 (epoch 0), train_loss = 1.634, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m985/1352 (epoch 0), train_loss = 1.722, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m986/1352 (epoch 0), train_loss = 1.611, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m987/1352 (epoch 0), train_loss = 1.669, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m988/1352 (epoch 0), train_loss = 1.613, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m989/1352 (epoch 0), train_loss = 1.573, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m990/1352 (epoch 0), train_loss = 1.652, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m991/1352 (epoch 0), train_loss = 1.584, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m992/1352 (epoch 0), train_loss = 1.675, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m993/1352 (epoch 0), train_loss = 1.598, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m994/1352 (epoch 0), train_loss = 1.630, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m995/1352 (epoch 0), train_loss = 1.621, time/batch = 0.206\u001b[0m\n",
      "\u001b[34m996/1352 (epoch 0), train_loss = 1.626, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m997/1352 (epoch 0), train_loss = 1.683, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m998/1352 (epoch 0), train_loss = 1.562, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m999/1352 (epoch 0), train_loss = 1.547, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1000/1352 (epoch 0), train_loss = 1.626, time/batch = 0.187\u001b[0m\n",
      "\u001b[34mmodel saved to s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model/model.ckpt\u001b[0m\n",
      "\u001b[34m1001/1352 (epoch 0), train_loss = 1.650, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m1002/1352 (epoch 0), train_loss = 1.592, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m1003/1352 (epoch 0), train_loss = 1.631, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1004/1352 (epoch 0), train_loss = 1.572, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1005/1352 (epoch 0), train_loss = 1.607, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1006/1352 (epoch 0), train_loss = 1.591, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1007/1352 (epoch 0), train_loss = 1.577, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m1008/1352 (epoch 0), train_loss = 1.650, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1009/1352 (epoch 0), train_loss = 1.563, time/batch = 0.165\u001b[0m\n",
      "\u001b[34m1010/1352 (epoch 0), train_loss = 1.623, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1011/1352 (epoch 0), train_loss = 1.596, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1012/1352 (epoch 0), train_loss = 1.598, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1013/1352 (epoch 0), train_loss = 1.544, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1014/1352 (epoch 0), train_loss = 1.598, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1015/1352 (epoch 0), train_loss = 1.580, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1016/1352 (epoch 0), train_loss = 1.590, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1017/1352 (epoch 0), train_loss = 1.649, time/batch = 0.298\u001b[0m\n",
      "\u001b[34m1018/1352 (epoch 0), train_loss = 1.596, time/batch = 0.195\u001b[0m\n",
      "\u001b[34m1019/1352 (epoch 0), train_loss = 1.618, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1020/1352 (epoch 0), train_loss = 1.604, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1021/1352 (epoch 0), train_loss = 1.581, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1022/1352 (epoch 0), train_loss = 1.550, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1023/1352 (epoch 0), train_loss = 1.597, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1024/1352 (epoch 0), train_loss = 1.635, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m1025/1352 (epoch 0), train_loss = 1.560, time/batch = 0.211\u001b[0m\n",
      "\u001b[34m1026/1352 (epoch 0), train_loss = 1.623, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1027/1352 (epoch 0), train_loss = 1.568, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1028/1352 (epoch 0), train_loss = 1.683, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1029/1352 (epoch 0), train_loss = 1.700, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m1030/1352 (epoch 0), train_loss = 1.641, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1031/1352 (epoch 0), train_loss = 1.648, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1032/1352 (epoch 0), train_loss = 1.575, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1033/1352 (epoch 0), train_loss = 1.577, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1034/1352 (epoch 0), train_loss = 1.592, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1035/1352 (epoch 0), train_loss = 1.605, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m1036/1352 (epoch 0), train_loss = 1.561, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1037/1352 (epoch 0), train_loss = 1.580, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1038/1352 (epoch 0), train_loss = 1.598, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1039/1352 (epoch 0), train_loss = 1.606, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1040/1352 (epoch 0), train_loss = 1.613, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1041/1352 (epoch 0), train_loss = 1.572, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1042/1352 (epoch 0), train_loss = 1.630, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1043/1352 (epoch 0), train_loss = 1.526, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1044/1352 (epoch 0), train_loss = 1.567, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1045/1352 (epoch 0), train_loss = 1.567, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1046/1352 (epoch 0), train_loss = 1.601, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m1047/1352 (epoch 0), train_loss = 1.645, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1048/1352 (epoch 0), train_loss = 1.609, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1049/1352 (epoch 0), train_loss = 1.615, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1050/1352 (epoch 0), train_loss = 1.611, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1051/1352 (epoch 0), train_loss = 1.560, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1052/1352 (epoch 0), train_loss = 1.604, time/batch = 0.202\u001b[0m\n",
      "\u001b[34m1053/1352 (epoch 0), train_loss = 1.570, time/batch = 0.252\u001b[0m\n",
      "\u001b[34m1054/1352 (epoch 0), train_loss = 1.631, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1055/1352 (epoch 0), train_loss = 1.639, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1056/1352 (epoch 0), train_loss = 1.540, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1057/1352 (epoch 0), train_loss = 1.616, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m1058/1352 (epoch 0), train_loss = 1.660, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1059/1352 (epoch 0), train_loss = 1.578, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1060/1352 (epoch 0), train_loss = 1.590, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1061/1352 (epoch 0), train_loss = 1.619, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1062/1352 (epoch 0), train_loss = 1.591, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1063/1352 (epoch 0), train_loss = 1.539, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1064/1352 (epoch 0), train_loss = 1.593, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1065/1352 (epoch 0), train_loss = 1.596, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1066/1352 (epoch 0), train_loss = 1.548, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1067/1352 (epoch 0), train_loss = 1.658, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1068/1352 (epoch 0), train_loss = 1.563, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1069/1352 (epoch 0), train_loss = 1.599, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1070/1352 (epoch 0), train_loss = 1.622, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1071/1352 (epoch 0), train_loss = 1.626, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1072/1352 (epoch 0), train_loss = 1.584, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1073/1352 (epoch 0), train_loss = 1.590, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1074/1352 (epoch 0), train_loss = 1.571, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1075/1352 (epoch 0), train_loss = 1.584, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m1076/1352 (epoch 0), train_loss = 1.569, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1077/1352 (epoch 0), train_loss = 1.575, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1078/1352 (epoch 0), train_loss = 1.606, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1079/1352 (epoch 0), train_loss = 1.625, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1080/1352 (epoch 0), train_loss = 1.617, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1081/1352 (epoch 0), train_loss = 1.555, time/batch = 0.237\u001b[0m\n",
      "\u001b[34m1082/1352 (epoch 0), train_loss = 1.636, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1083/1352 (epoch 0), train_loss = 1.577, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1084/1352 (epoch 0), train_loss = 1.576, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1085/1352 (epoch 0), train_loss = 1.681, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1086/1352 (epoch 0), train_loss = 1.633, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m1087/1352 (epoch 0), train_loss = 1.524, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1088/1352 (epoch 0), train_loss = 1.558, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1089/1352 (epoch 0), train_loss = 1.567, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1090/1352 (epoch 0), train_loss = 1.618, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1091/1352 (epoch 0), train_loss = 1.608, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1092/1352 (epoch 0), train_loss = 1.627, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1093/1352 (epoch 0), train_loss = 1.554, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1094/1352 (epoch 0), train_loss = 1.598, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1095/1352 (epoch 0), train_loss = 1.605, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1096/1352 (epoch 0), train_loss = 1.589, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1097/1352 (epoch 0), train_loss = 1.638, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1098/1352 (epoch 0), train_loss = 1.564, time/batch = 0.196\u001b[0m\n",
      "\u001b[34m1099/1352 (epoch 0), train_loss = 1.537, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1100/1352 (epoch 0), train_loss = 1.571, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1101/1352 (epoch 0), train_loss = 1.562, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1102/1352 (epoch 0), train_loss = 1.550, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1103/1352 (epoch 0), train_loss = 1.543, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1104/1352 (epoch 0), train_loss = 1.659, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1105/1352 (epoch 0), train_loss = 1.565, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1106/1352 (epoch 0), train_loss = 1.563, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1107/1352 (epoch 0), train_loss = 1.598, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m1108/1352 (epoch 0), train_loss = 1.575, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1109/1352 (epoch 0), train_loss = 1.596, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m1110/1352 (epoch 0), train_loss = 1.583, time/batch = 0.354\u001b[0m\n",
      "\u001b[34m1111/1352 (epoch 0), train_loss = 1.595, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1112/1352 (epoch 0), train_loss = 1.649, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1113/1352 (epoch 0), train_loss = 1.644, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1114/1352 (epoch 0), train_loss = 1.592, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1115/1352 (epoch 0), train_loss = 1.697, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1116/1352 (epoch 0), train_loss = 1.552, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1117/1352 (epoch 0), train_loss = 1.621, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1118/1352 (epoch 0), train_loss = 1.583, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1119/1352 (epoch 0), train_loss = 1.666, time/batch = 0.268\u001b[0m\n",
      "\u001b[34m1120/1352 (epoch 0), train_loss = 1.635, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1121/1352 (epoch 0), train_loss = 1.615, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1122/1352 (epoch 0), train_loss = 1.692, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1123/1352 (epoch 0), train_loss = 1.560, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1124/1352 (epoch 0), train_loss = 1.561, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1125/1352 (epoch 0), train_loss = 1.585, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1126/1352 (epoch 0), train_loss = 1.618, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m1127/1352 (epoch 0), train_loss = 1.564, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1128/1352 (epoch 0), train_loss = 1.602, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1129/1352 (epoch 0), train_loss = 1.546, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1130/1352 (epoch 0), train_loss = 1.603, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1131/1352 (epoch 0), train_loss = 1.582, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m1132/1352 (epoch 0), train_loss = 1.571, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1133/1352 (epoch 0), train_loss = 1.608, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1134/1352 (epoch 0), train_loss = 1.650, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1135/1352 (epoch 0), train_loss = 1.627, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1136/1352 (epoch 0), train_loss = 1.607, time/batch = 0.194\u001b[0m\n",
      "\u001b[34m1137/1352 (epoch 0), train_loss = 1.599, time/batch = 0.212\u001b[0m\n",
      "\u001b[34m1138/1352 (epoch 0), train_loss = 1.608, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m1139/1352 (epoch 0), train_loss = 1.599, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1140/1352 (epoch 0), train_loss = 1.601, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1141/1352 (epoch 0), train_loss = 1.613, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m1142/1352 (epoch 0), train_loss = 1.628, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m1143/1352 (epoch 0), train_loss = 1.623, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1144/1352 (epoch 0), train_loss = 1.588, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1145/1352 (epoch 0), train_loss = 1.582, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1146/1352 (epoch 0), train_loss = 1.573, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1147/1352 (epoch 0), train_loss = 1.581, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1148/1352 (epoch 0), train_loss = 1.552, time/batch = 0.182\u001b[0m\n",
      "\u001b[34m1149/1352 (epoch 0), train_loss = 1.603, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1150/1352 (epoch 0), train_loss = 1.531, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1151/1352 (epoch 0), train_loss = 1.572, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m1152/1352 (epoch 0), train_loss = 1.564, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1153/1352 (epoch 0), train_loss = 1.545, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m1154/1352 (epoch 0), train_loss = 1.570, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1155/1352 (epoch 0), train_loss = 1.576, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1156/1352 (epoch 0), train_loss = 1.585, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1157/1352 (epoch 0), train_loss = 1.514, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1158/1352 (epoch 0), train_loss = 1.611, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1159/1352 (epoch 0), train_loss = 1.573, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m1160/1352 (epoch 0), train_loss = 1.565, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1161/1352 (epoch 0), train_loss = 1.579, time/batch = 0.193\u001b[0m\n",
      "\u001b[34m1162/1352 (epoch 0), train_loss = 1.527, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1163/1352 (epoch 0), train_loss = 1.545, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1164/1352 (epoch 0), train_loss = 1.610, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1165/1352 (epoch 0), train_loss = 1.529, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m1166/1352 (epoch 0), train_loss = 1.536, time/batch = 0.209\u001b[0m\n",
      "\u001b[34m1167/1352 (epoch 0), train_loss = 1.563, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1168/1352 (epoch 0), train_loss = 1.592, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1169/1352 (epoch 0), train_loss = 1.514, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1170/1352 (epoch 0), train_loss = 1.556, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m1171/1352 (epoch 0), train_loss = 1.546, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1172/1352 (epoch 0), train_loss = 1.545, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m1173/1352 (epoch 0), train_loss = 1.554, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1174/1352 (epoch 0), train_loss = 1.545, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1175/1352 (epoch 0), train_loss = 1.549, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1176/1352 (epoch 0), train_loss = 1.561, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1177/1352 (epoch 0), train_loss = 1.587, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1178/1352 (epoch 0), train_loss = 1.579, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1179/1352 (epoch 0), train_loss = 1.665, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1180/1352 (epoch 0), train_loss = 1.671, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1181/1352 (epoch 0), train_loss = 1.613, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1182/1352 (epoch 0), train_loss = 1.535, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1183/1352 (epoch 0), train_loss = 1.582, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1184/1352 (epoch 0), train_loss = 1.585, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1185/1352 (epoch 0), train_loss = 1.526, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1186/1352 (epoch 0), train_loss = 1.534, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1187/1352 (epoch 0), train_loss = 1.598, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1188/1352 (epoch 0), train_loss = 1.547, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1189/1352 (epoch 0), train_loss = 1.566, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1190/1352 (epoch 0), train_loss = 1.551, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1191/1352 (epoch 0), train_loss = 1.563, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1192/1352 (epoch 0), train_loss = 1.520, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1193/1352 (epoch 0), train_loss = 1.570, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m1194/1352 (epoch 0), train_loss = 1.557, time/batch = 0.241\u001b[0m\n",
      "\u001b[34m1195/1352 (epoch 0), train_loss = 1.517, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1196/1352 (epoch 0), train_loss = 1.589, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1197/1352 (epoch 0), train_loss = 1.589, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1198/1352 (epoch 0), train_loss = 1.584, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1199/1352 (epoch 0), train_loss = 1.550, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m1200/1352 (epoch 0), train_loss = 1.567, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1201/1352 (epoch 0), train_loss = 1.514, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1202/1352 (epoch 0), train_loss = 1.552, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1203/1352 (epoch 0), train_loss = 1.555, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1204/1352 (epoch 0), train_loss = 1.626, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1205/1352 (epoch 0), train_loss = 1.551, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m1206/1352 (epoch 0), train_loss = 1.502, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1207/1352 (epoch 0), train_loss = 1.558, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1208/1352 (epoch 0), train_loss = 1.538, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1209/1352 (epoch 0), train_loss = 1.516, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1210/1352 (epoch 0), train_loss = 1.554, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1211/1352 (epoch 0), train_loss = 1.508, time/batch = 0.188\u001b[0m\n",
      "\u001b[34m1212/1352 (epoch 0), train_loss = 1.519, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1213/1352 (epoch 0), train_loss = 1.601, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1214/1352 (epoch 0), train_loss = 1.589, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1215/1352 (epoch 0), train_loss = 1.520, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1216/1352 (epoch 0), train_loss = 1.544, time/batch = 0.190\u001b[0m\n",
      "\u001b[34m1217/1352 (epoch 0), train_loss = 1.548, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1218/1352 (epoch 0), train_loss = 1.564, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1219/1352 (epoch 0), train_loss = 1.576, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1220/1352 (epoch 0), train_loss = 1.569, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1221/1352 (epoch 0), train_loss = 1.565, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1222/1352 (epoch 0), train_loss = 1.596, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m1223/1352 (epoch 0), train_loss = 1.522, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1224/1352 (epoch 0), train_loss = 1.614, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1225/1352 (epoch 0), train_loss = 1.601, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1226/1352 (epoch 0), train_loss = 1.562, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1227/1352 (epoch 0), train_loss = 1.575, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1228/1352 (epoch 0), train_loss = 1.576, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m1229/1352 (epoch 0), train_loss = 1.532, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1230/1352 (epoch 0), train_loss = 1.563, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1231/1352 (epoch 0), train_loss = 1.584, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1232/1352 (epoch 0), train_loss = 1.638, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1233/1352 (epoch 0), train_loss = 1.601, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1234/1352 (epoch 0), train_loss = 1.530, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1235/1352 (epoch 0), train_loss = 1.564, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1236/1352 (epoch 0), train_loss = 1.557, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1237/1352 (epoch 0), train_loss = 1.555, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1238/1352 (epoch 0), train_loss = 1.540, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1239/1352 (epoch 0), train_loss = 1.538, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1240/1352 (epoch 0), train_loss = 1.593, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1241/1352 (epoch 0), train_loss = 1.517, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1242/1352 (epoch 0), train_loss = 1.569, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1243/1352 (epoch 0), train_loss = 1.599, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1244/1352 (epoch 0), train_loss = 1.708, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1245/1352 (epoch 0), train_loss = 1.568, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m1246/1352 (epoch 0), train_loss = 1.514, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1247/1352 (epoch 0), train_loss = 1.576, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1248/1352 (epoch 0), train_loss = 1.572, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1249/1352 (epoch 0), train_loss = 1.598, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1250/1352 (epoch 0), train_loss = 1.534, time/batch = 0.180\u001b[0m\n",
      "\u001b[34m1251/1352 (epoch 0), train_loss = 1.618, time/batch = 0.192\u001b[0m\n",
      "\u001b[34m1252/1352 (epoch 0), train_loss = 1.513, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1253/1352 (epoch 0), train_loss = 1.615, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1254/1352 (epoch 0), train_loss = 1.570, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1255/1352 (epoch 0), train_loss = 1.600, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1256/1352 (epoch 0), train_loss = 1.595, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1257/1352 (epoch 0), train_loss = 1.562, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1258/1352 (epoch 0), train_loss = 1.609, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1259/1352 (epoch 0), train_loss = 1.542, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1260/1352 (epoch 0), train_loss = 1.557, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1261/1352 (epoch 0), train_loss = 1.492, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1262/1352 (epoch 0), train_loss = 1.601, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1263/1352 (epoch 0), train_loss = 1.613, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1264/1352 (epoch 0), train_loss = 1.624, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1265/1352 (epoch 0), train_loss = 1.550, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1266/1352 (epoch 0), train_loss = 1.682, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1267/1352 (epoch 0), train_loss = 1.602, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1268/1352 (epoch 0), train_loss = 1.507, time/batch = 0.186\u001b[0m\n",
      "\u001b[34m1269/1352 (epoch 0), train_loss = 1.585, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1270/1352 (epoch 0), train_loss = 1.552, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1271/1352 (epoch 0), train_loss = 1.523, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1272/1352 (epoch 0), train_loss = 1.578, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1273/1352 (epoch 0), train_loss = 1.579, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1274/1352 (epoch 0), train_loss = 1.596, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1275/1352 (epoch 0), train_loss = 1.627, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1276/1352 (epoch 0), train_loss = 1.599, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1277/1352 (epoch 0), train_loss = 1.554, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1278/1352 (epoch 0), train_loss = 1.521, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1279/1352 (epoch 0), train_loss = 1.503, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1280/1352 (epoch 0), train_loss = 1.540, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m1281/1352 (epoch 0), train_loss = 1.679, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1282/1352 (epoch 0), train_loss = 1.542, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1283/1352 (epoch 0), train_loss = 1.571, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1284/1352 (epoch 0), train_loss = 1.585, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1285/1352 (epoch 0), train_loss = 1.550, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1286/1352 (epoch 0), train_loss = 1.508, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1287/1352 (epoch 0), train_loss = 1.578, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1288/1352 (epoch 0), train_loss = 1.553, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1289/1352 (epoch 0), train_loss = 1.600, time/batch = 0.229\u001b[0m\n",
      "\u001b[34m1290/1352 (epoch 0), train_loss = 1.635, time/batch = 0.205\u001b[0m\n",
      "\u001b[34m1291/1352 (epoch 0), train_loss = 1.595, time/batch = 0.189\u001b[0m\n",
      "\u001b[34m1292/1352 (epoch 0), train_loss = 1.527, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1293/1352 (epoch 0), train_loss = 1.563, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1294/1352 (epoch 0), train_loss = 1.561, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1295/1352 (epoch 0), train_loss = 1.495, time/batch = 0.179\u001b[0m\n",
      "\u001b[34m1296/1352 (epoch 0), train_loss = 1.522, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1297/1352 (epoch 0), train_loss = 1.534, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1298/1352 (epoch 0), train_loss = 1.500, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1299/1352 (epoch 0), train_loss = 1.575, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1300/1352 (epoch 0), train_loss = 1.610, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1301/1352 (epoch 0), train_loss = 1.555, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1302/1352 (epoch 0), train_loss = 1.499, time/batch = 0.185\u001b[0m\n",
      "\u001b[34m1303/1352 (epoch 0), train_loss = 1.535, time/batch = 0.198\u001b[0m\n",
      "\u001b[34m1304/1352 (epoch 0), train_loss = 1.564, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1305/1352 (epoch 0), train_loss = 1.506, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1306/1352 (epoch 0), train_loss = 1.522, time/batch = 0.167\u001b[0m\n",
      "\u001b[34m1307/1352 (epoch 0), train_loss = 1.544, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1308/1352 (epoch 0), train_loss = 1.553, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1309/1352 (epoch 0), train_loss = 1.539, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1310/1352 (epoch 0), train_loss = 1.554, time/batch = 0.168\u001b[0m\n",
      "\u001b[34m1311/1352 (epoch 0), train_loss = 1.574, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1312/1352 (epoch 0), train_loss = 1.575, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1313/1352 (epoch 0), train_loss = 1.547, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1314/1352 (epoch 0), train_loss = 1.583, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1315/1352 (epoch 0), train_loss = 1.545, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1316/1352 (epoch 0), train_loss = 1.560, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1317/1352 (epoch 0), train_loss = 1.572, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1318/1352 (epoch 0), train_loss = 1.548, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1319/1352 (epoch 0), train_loss = 1.564, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1320/1352 (epoch 0), train_loss = 1.541, time/batch = 0.191\u001b[0m\n",
      "\u001b[34m1321/1352 (epoch 0), train_loss = 1.568, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1322/1352 (epoch 0), train_loss = 1.503, time/batch = 0.170\u001b[0m\n",
      "\u001b[34m1323/1352 (epoch 0), train_loss = 1.539, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1324/1352 (epoch 0), train_loss = 1.606, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1325/1352 (epoch 0), train_loss = 1.513, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1326/1352 (epoch 0), train_loss = 1.555, time/batch = 0.183\u001b[0m\n",
      "\u001b[34m1327/1352 (epoch 0), train_loss = 1.558, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1328/1352 (epoch 0), train_loss = 1.526, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1329/1352 (epoch 0), train_loss = 1.583, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1330/1352 (epoch 0), train_loss = 1.593, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1331/1352 (epoch 0), train_loss = 1.578, time/batch = 0.206\u001b[0m\n",
      "\u001b[34m1332/1352 (epoch 0), train_loss = 1.542, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1333/1352 (epoch 0), train_loss = 1.542, time/batch = 0.177\u001b[0m\n",
      "\u001b[34m1334/1352 (epoch 0), train_loss = 1.535, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1335/1352 (epoch 0), train_loss = 1.564, time/batch = 0.181\u001b[0m\n",
      "\u001b[34m1336/1352 (epoch 0), train_loss = 1.570, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1337/1352 (epoch 0), train_loss = 1.540, time/batch = 0.187\u001b[0m\n",
      "\u001b[34m1338/1352 (epoch 0), train_loss = 1.584, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1339/1352 (epoch 0), train_loss = 1.545, time/batch = 0.176\u001b[0m\n",
      "\u001b[34m1340/1352 (epoch 0), train_loss = 1.559, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1341/1352 (epoch 0), train_loss = 1.558, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1342/1352 (epoch 0), train_loss = 1.576, time/batch = 0.171\u001b[0m\n",
      "\u001b[34m1343/1352 (epoch 0), train_loss = 1.541, time/batch = 0.184\u001b[0m\n",
      "\u001b[34m1344/1352 (epoch 0), train_loss = 1.546, time/batch = 0.174\u001b[0m\n",
      "\u001b[34m1345/1352 (epoch 0), train_loss = 1.561, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1346/1352 (epoch 0), train_loss = 1.516, time/batch = 0.175\u001b[0m\n",
      "\u001b[34m1347/1352 (epoch 0), train_loss = 1.575, time/batch = 0.169\u001b[0m\n",
      "\u001b[34m1348/1352 (epoch 0), train_loss = 1.606, time/batch = 0.172\u001b[0m\n",
      "\u001b[34m1349/1352 (epoch 0), train_loss = 1.657, time/batch = 0.178\u001b[0m\n",
      "\u001b[34m1350/1352 (epoch 0), train_loss = 1.563, time/batch = 0.173\u001b[0m\n",
      "\u001b[34m1351/1352 (epoch 0), train_loss = 1.564, time/batch = 0.170\u001b[0m\n",
      "\u001b[34mmodel saved to s3://sagemaker-us-west-2-517141035927/tensorflow-training-2020-12-16-08-17-17-586/model/model.ckpt\u001b[0m\n",
      "\u001b[34m2020-12-16 08:25:02,376 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2020-12-16 08:25:02,376 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-12-16 08:25:46 Uploading - Uploading generated training model\n",
      "2020-12-16 08:25:46 Completed - Training job completed\n",
      "Training seconds: 347\n",
      "Billable seconds: 347\n"
     ]
    }
   ],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='char-rnn-tensorflow',\n",
    "                       git_config=git_config,\n",
    "                       train_instance_type='ml.c4.xlarge', # Executes training in a ml.c4.xlarge instance\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version='1.15.2',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "             \n",
    "\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.1-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
